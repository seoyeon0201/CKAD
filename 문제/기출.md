## Q1. 시크릿을 생성하고 `시크릿의 값을 pod의 환경변수로 가져오기`

1. Secret 생성
- `dotfile-secret`이라는 이름의 secret 생성

`kubectl create secret generic [SECRET NAME] [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run=server|client|none] [options]`

2. Pod의 환경변수로 가져오기
- env.valueFrome.secretKeyRef 사용

>해결

`kubectl create secret generic backend-user --from-literal=backend-username='backend-admin'`

`pod.yaml`
```
apiVersion: v1
kind: Pod
metadata:
  name: env-single-secret
spec:
  containers:
  - name: envars-test-container
    image: nginx
    env:
    - name: SECRET_USERNAME
      valueFrom:
        secretKeyRef:
          name: backend-user  #secret name
          key: backend-username #넣고자하는 secret의 key값
```

>확인

- `kubectl exec -i -t [POD NAME] -- /bin/sh -c 'echo $[원하는 ENV NAME]'`



## Q2. resource-request를 이용한 pod 리소스 제한

- spec.containers.resources.requests 또는 spec.containers.resources.limits와 같이 container 내에서 requests와 limits 설정해 리소스 관리

`pod.yaml`
```
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:  
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
```

>확인
- `kubectl describe pod [POD NAME]`

## Q3. readiness Probe 추가

- spec.containers.readinessProbe 또는 spec.containers.livenessProbe와 같이 container 내에서 설정

`pod.yaml`
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
    livenessProbe:  #또는 readinessProbe
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
```

>확인
- `kubectl describe pod [POD NAME]`

## Q4. service account를 pod에 추가

1. ServiceAccount 생성

- sa1이라는 이름의 serviceaccount 생성

`kubectl create serviceaccount sa1 [--dry-run=server|client|none] [options]`

2. Pod에 적용

`pod.yaml`
```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod2
  name: pod2
spec:
  serviceAccountName: sa1     #serviceaccount 적용
  automountServiceAccountToken: false
  containers:
  - image: nginx
    name: pod2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

## Q5. docker 이미지 빌드하고 tar파일로 save 하는 문제

#### Q1. Docker 이미지 빌드:
#### ⭐️ A1
    - Dockerfile이 `/data/ckad/build/apache-php/Dockerfile`에 위치한다고 가정합니다.
    - 다음 명령어를 사용하여 Docker 이미지를 빌드합니다:
        
        ```bash
        docker build -t apache-php:ckad-v1 /data/ckad/build/apache-php
        ```
        
    - 이 명령어는 `apache-php`라는 이미지 이름과 `ckad-v1`이라는 태그를 가진 이미지를 빌드합니다.

#### Q2. Docker 이미지 저장:

#### ⭐️ A2
    - 다음 명령어를 사용하여 빌드한 Docker 이미지를 tar 파일로 저장합니다:
        
        ```bash
        docker save -o /data/ckad/apache-php-ckad-v1.tar apache-php:ckad-v1
        ```
        
    - 이 명령어는 `apache-php:ckad-v1` 이미지를 `apache-php-ckad-v1.tar`라는 파일로 저장합니다.
    - `o` 옵션은 출력 파일을 지정하는 데 사용되며, 파일 경로는 `/data/ckad/apache-php-ckad-v1.tar`로 설정되어 있습니다.

## Q6. depoly 버전업되어서 yaml 수정하기

⭐️ api랑 selector 등을 수정해야 함

## Q7. deploy yaml 수정하고 롤링업데이트 진행하기

- `--record`를 하지 않는 경우, `k rollout history deployment deploy-rollout` 명령에 대해서 변경사유가 표시되지 않음

```bash
# rolling update
kubectl set image deployment eshop-payment nginx=nginx:1.17 --record  

kubectl rollout history deployment eshop-payment

# rollback
kubectl rollout undo deployment eshop-payment
```

## Q8. 카나리 배포중에 current와 canary가 있는데 canary가 트래픽의 30퍼센트 받도록 service를 수정

#### ⭐️ 강의에 카나리 배포 내용 다시 듣기

- Max파드가 10개고 canary가 30%정도 비율이여야 함
    1. canary 디플로이를 만들고 current와 공유하는 label을 붙인다.
    2. current pod를 7개, canary pod를 3개로 설정한다.
    3. 서비스가 current와 canary가 공유하는 label을 select하도록 설정한다.

1. Service의 label이 current pod와 canary pod의 label로 존재해야함
  - 하나의 service로 current pod와 canary pod 연결
  - deployment 또는 replicaset이 아닌 pod에 존재하는 label과 동일해야함 
2. Current deployment에 replicas를 7, Canary deployment에 replicas를 3로 설정해 트래픽 분산

## Q9. pod의 security context에서 권한 승격불가능하고, uid 20000으로 사용

`pod.yaml`
```
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: 20000
  volumes:
  - name: sec-ctx-vol
    emptyDir: {}
  containers:
  - name: sec-ctx-demo
    image: busybox:1.28
    command: [ "sh", "-c", "sleep 1h" ]
    volumeMounts:
    - name: sec-ctx-vol
      mountPath: /data/demo
    securityContext:
      allowPrivilegeEscalation: false
```

- 이때 pod의 uid를 20000으로 하는지, container의 uid를 20000으로 하는지 유의하기

>확인
- `ps`

## Q10. networkpolicy newpod가 db, web 하고만 통신가능하도록 수정

⭐️ 강의 다시 듣고 문제도 다시 풀기

⭐️ 네트워크 팔러시 절대 건들지 말고 접근해야하는 파드만 고칠것

>네트워크 정책 4개 있음
>1. all-access → db, web 파드에 붙이기
>2. default deny
>3. web access → new pod 에 붙이기
>4. db access → new pod 에 붙이기

- 각 network policy의 label과 동일하게 pod의 label 수정 !

`networkpolicy.yaml`
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
```

⭐️ db, web 파드는 all-access netpol 붙이고 newpod는 db, web과 통신할 수 있는 netpol과 연결하도록 label 수정한다

- db, web pod는 all-access netpol 내의 label과 일치시키고, newpod는 web access와 db access 내의 label과 일치시킴
- Service에서 pod name과 label이 일치되었는지도 확인해야함

## Q11. ingress / service / deploy가 주어졌을 때, deploy 수정하지 않고, service와 ingress만 수정해서 응답값을 받아오는 문제

> - 주어진 url에 맞게 host, path 값을 수정
> - service 이름도 수정
> - 파드의 컨테이너 포트와 서비스의 포트, ingress가 연결되도록 포트 설정 꼼꼼히 할 것
> - curl로 테스트했을때 html 문서가 출력되어야 함

- 이때 service name과 port 확인해야함

- /path별 또는 hostname 사용
- `metadata.annotations.nginx.ingress.kubernetes.io/rewrite-target: /` 사용


## Q12. limits 문제

문제 : pod가 실행이 안되는데 request 값을 맞춰주고, limits 값는 namespace에서 제한한 값의 절반으로 해야함.

⭐️ **해당 namespace의 limit range를 살펴보고** 그것의 절반값으로 설정해야 함

- `k get limitranges`

> limit range는 namespace 단위로 적용된다. 아래 예시
`limitrange.yaml`
```
apiVersion: v1
kind: LimitRange
metadata:
  name: my-limit-range
spec:
  limits:
  - type: Container
    defaultRequest:
      cpu: 100m
      memory: 256Mi
    defaultLimit:
      cpu: 200m
      memory: 512Mi
    maxLimitRequestRatio:
      cpu: 4
      memory: 2
    min:
      cpu: 50m
      memory: 128Mi
    max:
      cpu: 500m
      memory: 1Gi
```

## Q13. Cronjob 만들기

> - 주어진 pod에 대한 컨테이너 정보
> - 5번의 성공
> - 2번의 실패 허용
> - 10분마다 진행
> - 파드는 8초 뒤에 삭제
> - 크론잡의 템플릿을 복사해 다른 잡을 만들고 실행해야 함

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: my-cronjob
spec:
  schedule: "*/10 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: my-container
            image: your-image:latest
            command: ["sh", "-c", "your-command"]
          restartPolicy: OnFailure
  backoffLimit: 2   # 최대 리트라이 횟수
  completions: 5     # 성공해야 하는 작업의 수
  activeDeadlineSeconds: 8  # 최대 실행 시간 (초)
```

## Q14. Taint and Tolerant 문제 존재하는 pod와 deployment 존재


⭐️ Kubernetes 공식 페이지의 `Cheatsheet` 보기

⭐️ `kubectl config set-context --current --namespace=[바꾸고자하는 NAMESPACE명]`

- `k config get-contexts`로 현재 context 확인

- 시험 볼 때 context 계속해서 바뀌는 것 유의하고, namespace도 문제에 나타난 namespace로 반드시 지정해야함

⭐️ command로 리소스 생성

- `k run [POD NAME] [OPTIONS]`

⭐️ grep 사용

- `k describe pod [POD NAME] | grep image -A10`
  - image라는 단어가 있는 곳의 앞 10줄을 찾아 출력