## Rolling Update

- Recreate 전략과 달리 구버전을 하나씩 내리고 신버전을 하나씩 순차적인 올리는 방식 사용

- `k create -f [YAML FILE] --record`

  - 리소스 생성하면서 동시에 기록
  - 생성시 `--record`하지 않으면 `k rollout history deployment/[DEPLOYMENT NAME]`에 보이지 않음

- `k rollout statue deployment/[DEPLOYMENT NAME]`
  - rollout되는 상태 지켜봄
- `k rollout history deployment/[DEPLOYMENT NAME]`

  - history 조회

- Rolling update 방식

  - 1. YAML 파일 수정
    - YAML 파일에서 image 수정 후 `k apply -f [YAML FILE]`
  - 2. 명령어로 업데이트
    - `k set image deployment/[DEPLOYMENT NAME] [CONTAINER NAME]=[변경할 image]`

- `k rollout undo deployment/[DEPLOYMENT NAME]`
  - 이전 버전으로 rollback

## Blue/Green

- version:v1의 label을 가진 deployment와 version:v1을 selector하는 service 존재
- version:v2의 label을 가진 deployment 생성
- service의 selector을 version: v1에서 version: v2로 변경

## Canary

- 새 버전의 deployment를 배포해 소량의 트래픽만 경로 지정
  - 대부분의 트래픽은 구버전으로 가지만 소량의 일부 트래픽은 신버전으로 감
- 정상적으로 동작하면 구버전을 신버전으로 변경
  - rolling update 방식으로 변경
- 처음에 소량의 트래픽 경로로 지정한 canary 제거

#### 해결해야하는 문제

1. 구버전과 신버전의 deployment로 동시에 트래픽 전달해줘야함

- 구버전과 신버전이 `label을 공통`으로 가지도록 설정하고, `service는 공통으로 갖는 label을 selector 매핑`

  - 이때 트래픽 비율을 결정하고 싶은 경우, service의 selector와 deployment의 pod의 label이 맞아야 함 !

2. 신버전에는 작은 퍼센트의 트래픽이 가야함

- traffic은 pod의 수에 따라 결정되므로 신버전의 `pod 수를 줄임`

## Secret

- Secret을 인코드하여 넣기 위해서 `echo -n '[SECRET의 VALUE값]' | base64`

- 디코드하여 값을 찾기 위해 `echo -n '[SECRET의 인코딩된 VALUE값]'|base64 --decode`

- secret은 아래와 같이 data의 value가 인코딩되어나타나 값을 알기 위해서는 디코딩해야함
  `secret.yaml`

```
apiVersion: v1
kind: Secret
metadata:
    name: app-secret
data:
    DB_Host: bXlzcWw=
    DB_User: cm9vdA==
    DB_Password: cGFzd3Jk
```

## Security Context

- Kubernetes는 container 실행시 기본으로 root 권한으로 실행되는데, root 권한에서의 container 실행을 방지하기 위해 실행시킬 PID 지정
- pod 또는 container에 대한 권한 및 액세스 제어 설정 정의
- spec.securityContext인 경우 kind에 해당하는 리소스 전체에 권한 및 액세스 제어가 가능하고, spec.containers.securityContext의 경우 해당 container내에서 권한 및 액세스 제어를 할 수 있음

- Option
  - `runAsUser`: pod 또는 container를 실행시킬 PID 지정
  - `runAsGroup`: pod 또는 container를 실행시킬 GID 지정
  - `fsGroup`: volumeMount시 활용할 PID 지정
  - `runAsNonRoot`: container를 root가 아닌 사용자로 실행할지 지정

## Service Account

- Service Account

  - k8s에서 애플리케이션의 인증 및 권한 관리를 담당하는 객체
  - (과거) Kubernetes에는 기본 service account가 namespace마다 존재하고, 해당 service account에는 token을 가지는 secret object 존재
  - (과거) Token이 생성되고 secret object를 생성해 그 안에 token 저장
    - secert object는 service account에 링크
  - 과거에는 토큰이 자동으로 생성되어 secret에 저장되었지만, 현재는 `k create token [SERVICE ACCOUNT NAME]`으로 토큰을 직접 생성해야함

- Service Account Ex

  - 응용 프로그램이 쿠버네티스 API를 쿼리하려면 Authentication 필요한데, 이때 service account 사용

- Use
  - `k create sa [SERVICE ACCOUNT NAME]`
  - `automountServiceAccountToken: false`로 service account 자동 마운트하지 않을 수 있음
  - spec.template.spec.`serviceAccountName`에 생성한 service account name 작성

## Taints and Tolerations

- 한 node에 어떤 pod를 scheduling 할지 제한
- Node에 Taint를 두고 pod에 Toleration 설정하여 특정 node의 taint를 견딜 수 있는 toleration이 있는 pod만 해당 node에 위치할 수 있음

- Taint와 Toleration은 해당 node로의 진입을 "허락"해주는 것이지, 반드시 그 node로 가야하는 것은 아님 !

  - 해당 node에 위치시키고 싶은 것은 node affinity 사용

- Use

  - Node에 Taint: `k taint nodes [NODE NAME] [KEY]=[VALUE]:[TAINT EFFECT]`
    - taint effect는 pod가 taint에 tolerate되지 않는 경우 처리하는 방법을 나타냄
    - NoSchedule, PreferNoSchedule, NoExecute
      - NoSchedule: 기존의 배치에서 변경될 때 node에 taint가 생기고 pod에 toleration이 생기지 않은 경우 pod가 죽음
      -
  - Pod에 Toleration: YAML 파일 수정

    - taint와 동일한 값을 spec.tolerations에 작성
    - 이때 모든 value들은 문자열 형태여야함

  - Node의 Taint 삭제: `k taint node [NODENAME] [기존의 TAINT 복붙]-`
    - "-"을 붙여야 삭제 명령어

## Node Selector

- pod가 특정 node에서 동작하도록 설정
- 1. node에 label을 붙이기
  - `k labels nodes [NODE NAME] [LABEL KEY]=[LABEL VALUE]`
- 2.  pod 정의 파일에서 spec.nodeSelector에 해당 label 작성해 매핑
  - `pod.yaml`
  ```
  ...
  spec:
    nodeSelector: #node의 label과 매핑
      [LABEL KEY]: [LABEL VALUE]
  ```

## Node Affinity

- label에 따라 특정 node에 pod 배치 제한하는 고급 기능

- Node Affinity Types

  - `requiredDuringSchedulingIgnoredDuringExecution`
    - 조건에 일치하는 node가 없는 경우 pod가 scheduling되지 않음
    - 일단 pod schedule이 되면 이후에 변화가 있어도 schedule이 변하지 않음
  - `preferredDuringSchedulingIgnoredDuringExecution`
    - 조건에 일치하는 node가 없는 경우 pod가 아무 node에 scheduling됨
    - 일단 pod schedule이 되면 이후에 변화가 있어도 schedule이 변하지 않음
  - `requiredDuringSchedulingRequiredDuringExecution`
    - 조건에 일치하는 node가 없는 경우 pod가 scheduling되지 않음
    - pod schedule이 되어도 실행 중 변동사항이 생겨 pod가 원하는 곳에 있지 않으면 종료되거나 퇴출

  |       | DuringScheduling | DuringExecution |
  | :---: | :--------------: | :-------------: |
  | Type1 |     Required     |     Ignored     |
  | Type2 |    Preferred     |     Ignored     |
  | Type3 |     Required     |    Required     |

## Readiness and Liveness

- Pod Status

  - Pending
  - ContainerCreating
  - Running

- Pod Condition
  - PodScheduled
    - node 지정되었는가
  - Initialized
  - ContainersReady
  - Ready

#### Readiness

- Pod가 Ready 상태가 되면 Service는 Pod로 트래픽 전송
- 하지만 Pod는 응용 프로그램이 실행되지 않은 단순히 Ready 상태에 있는 상태
  - 이때 트래픽이 전송되면 사용자는 애플리케이션을 정상적으로 접근할 수 없음
- 따라서 Ready condition이 pod 내부의 응용 프로그램 실제 상태에 연결되어야 함

- Pod 내부의 응용 프로그램이 정상적으로 동작하는 경우 Ready 상태로 변환!
  - 이때 내부 프로그램이 정상적으로 동작하는 것은 `HTTP Test`, `TCP Test`, `Exec command`로 확인할 수 있음

```
readinessProbe:
  #HTTP Test
  httpGet:
    path: /api/ready
    port: 8080

  #TCP Test
  tcpSocket:
    port: 3306

  #Exec command
  exec:
    command:
    - cat
    - /app/is_ready
```

- Option

  - `initialDelaySeconds`
    - 초기 딜레이 시간
  - `periodSeconds`
    - 동작 탐색 주기
  - `failureThreshold`
    - 지정한 횟수만큼 계속 시도

#### Liveness

- container는 살아있지만 내부의 애플리케이션이 작동하지 않는다면 container를 재가동하거나 삭제하고 새로운 container를 생성해야함
- Liveness Probe는 컨테이너에서 정기적으로 구성되어 컨테이너 내의 응용 프로그램이 정상인지 테스트
  - 테스트 결과 실패면 컨테이너가 unhealthy하다고 생각해 제거하고 재생성
- Pod 내부의 응용 프로그램이 정상적으로 동작하지 않으면 container 삭제 후 재생성
  - 이때 내부 프로그램이 정상적으로 동작하는 것은 `HTTP Test`, `TCP Test`, `Exec command`로 확인할 수 있음

```
livenessProbe:
  #HTTP Test
  httpGet:
    path: /api/ready
    port: 8080

  #TCP Test
  tcpSocket:
    port: 3306

  #Exec command
  exec:
    command:
    - cat
    - /app/is_ready
```

## Job

- `restartPolicy`

  - Always : 항상 재시작
  - OnFailure : 비정상 종료 발생시 컨테이너 재시작
  - Never : 재시작하지 않음

- `backoffLimit`: job에 실패할 경우 다시 실행시킬 재시도 횟수. 기본으로 6
  - job YAML 파일 생성시 backoffLimit 설정 안 하면 기본으로 6번 시도하고 종료
  - 따라서 backoffLimit을 "최대한 크게 설정"할 것!
- `completions`: 성공할 pod 횟수
- `parallelism`: 하나의 pod에 여러 container를 실행시킴

## Ingress

- 사용자가 외부적으로 액세스 가능한 단일 URL을 이용해 응용 프로그램에 액세스하도록 도움
- URL 경로로 다양한 서비스로 라우트

- Ingress를 사용할 때에는 Ingress Controller와 Ingress Resource가 필요

  - Ingress Controller: NGINX, HAPROXY, traefik
  - Ingress Resource: YAML 파일

- Ingress Resource
- 1. 단일 서비스와 연결
     `Ingress-wear.yaml`

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear
spec:
  backend:
    serviceName: wear-service
    servicePort: 80
```

- 2. path 별로
  - `my-online-store.com/wear`, `my-online-store.com/watch`
    `ingress.yaml`

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx-example
  rules:
  - http:
      paths:
      - path: /testpath
        pathType: Prefix
        backend:
          service:
            name: test
            port:
              number: 80
```

- 3. 호스트나 도메인 이름 별로
  - `wear.my-online-store.com`, `watch.my-online-store.com`

`ingress.yaml`

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wildcard-host
spec:
  rules:
  - host: "foo.bar.com"
    http:
      paths:
      - pathType: Prefix
        path: "/bar"
        backend:
          service:
            name: service1
            port:
              number: 80
  - host: "*.foo.com"
    http:
      paths:
      - pathType: Prefix
        path: "/foo"
        backend:
          service:
            name: service2
            port:
              number: 80
```

- 각 호스트나 도메인 이름에 규칙이 있고 각각의 규칙 안에 URL에 기반한 트래픽 경로 존재

- Rule 1. www.my-online-store.com

  - `http://www.my-online-store.com/wear`, `http://www.my-online-store.com/watch`

  - Path: /wear, /watch

- Rule 2. www.wear.my-online-store.com

  - `http://www.wear.my-online-store.com/`, `http://www.wear.my-online-store.com/returns`, `http://www.wear.my-online-store.com/support`
  - Path: /, /returns, /support

- Rule 3. www.watch.my-online-store.com

  - `http://www.watch.my-online-store.com/`, `http://www.watch.my-online-store.com/movies`, `http://www.watch.my-online-store.com/tv`
  - Path: /, /movies, /tv

- Rule 4. Everything Else

  - www.wear.my-online-store.com
  - `http://www.listen.my-online-store.com/`, `http://www.eat.my-online-store.com/`, `http://www.drink.my-online-store.com/tv`

- `metadata.annotations.nginx.ingress.kubernetes.io/rewrite-target: /` 반드시 사용

  - 해당 옵션을 사용하면 `http://<ingress-service>:<ingress-port>/watch` --> `http://<watch-service>:<port>/`로 변환
  - 사용하지 않은 경우 `http://<ingress-service>:<ingress-port>/watch` --> `http://<watch-service>:<port>/watch`로 뒤에 path가 붙음

- `ingressClassName: nginx-example`은 반드시 제거

## Network Policies

- 기본적으로 pod는 node가 달라도 항상 연결될 수 있도록 설계되어있는데, network policy를 이용해 특정 Pod에서의 ingress와 egress만을 허용할 수 있음
- Network Policy는 Kubernetes namespace의 Object로, 하나 이상의 pod에 링크하고 policy 내에서 규칙 정의 가능
- Network policy가 생성되면 모든 pod의 모든 트래픽을 차단하고, 지정된 규칙에 부합하는 트래픽만 허용

- Use
  - pod에 label을 붙이고 network policy의 podSelector field에 같은 label을 사용

`networkpolicy.yaml`

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db  #현재 pod를 나타내는 label
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:  #해당 IP로 들어오는 트래픽 허용
        cidr: 172.17.0.0/16
    - namespaceSelector:  #해당 네임스페이스에 존재하는 리소스로부터의 트래픽 허용
        matchLabels:
          project: myproject
    - podSelector:  #해당 pod로부터 들어오는 트래픽 허용
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379  #들어오는 트래픽(현재 pod) 포트번호
  egress:
  - to:
    - ipBlock:  #해당 IP로 나가는 트래픽 허용
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978  #나가는 트래픽 포트번호
```
